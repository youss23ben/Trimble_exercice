You can find in this file my summary of the following paper intiteld:
An image is worth 16x16 words:
Transformers for image recognition at scale
https://arxiv.org/pdf/2010.11929.pdf

--> summary:
The paper introduces a new neural network architecture for image classification which is based on the transformer architecture. It is called ViT (Vision Transformer) and uses self attention to model the interactions between different parts of an image.

The model applies global attention by dividing images into patches. They are obtained by dividing the input image into a grid of fixed-size patches. Then each patch is flattered into a vector. Each vector is then multiplied with an embedding matrix to give patch embedding.
Along with patch embedding a class token is created, which is similar to the CLS token in Bert. Because the transformer doesnâ€™t know the order of the input a positional encoding vector is then added to the patch embedding to include spatial information. The sequence of patch vectors is then fed into the encoder transformer, which consists of a stack of self-attention, normalization and multilayer perceptron layers. The output of the class token embedding (learnable embedding) is the result of the classification.

Compared to CNN, ViT has less image specific inductive bias because only MLP (Multi layer perceptron) are local while self attention layers are global. ViT uses dimensional neighborhood structure twice, in the beginning when cutting images into patches and at fine tuning when adjusting the position embeddings for images of different resolutions.

There is a hybrid architecture to ViT where the image patches are extracted from a CNN feature map as an alternative to raw image patches.

There are variants of the ViT architecture which are based on those used for Bert, including Base, Large. A Huge model was also added using 32 layers and 623M parameters. The input patch size is also a variant for the model.

The model ViT is trained using Adam with weight decay and a linear learning rate warmup and decay. For fine tuning SGD with momentum is used. The models are evaluated and compared to traditional CNN on various image classification benchmarks such as ImageNet, CIFAR-100, Oxford Flowers and VTAB. It performs better than Big CNN based networks such as ResNet on those benchmarks. It is also faster and more efficient in terms of computational costs.

ViT is also much faster when used with bigger patch sizes, in that smaller sequence of patches. Experiments shows that ViT using patches of 16*16 is 4 times faster than with patches of size 14*14.

Though CNN models outperform transformer models when pretrained with a small dataset. ViT only performs better on big datasets.


--> why this paper what interesting to me?

The transformer architecture has revolutionized NLP. Self attention has been quite successful in natural language processing and speech recognition.
So far images were a difficult domain for transformers to operate in because self attention would be calculated for each couple of pixels and it is computationally heavy.
There have been some ideas to overcome this issue and reduce the number of times self attention is applied, for example applying attention locally for each query pixel, or the use of transformers in combination with CNN.
I felt this paper was interesting and important publication in the field of computer vision. It introduced a new architecture (ViT) that uses self-attention mechanisms to process image data without relying on convolutional layers.
ViT's success challenges the common belief that convolutional layers are necessary for effective image processing and opens up new possibilities for using self-attention mechanisms in computer vision tasks. The paper also explores hybrid models that combine ViT with convolutional layers, demonstrating that the two approaches can complement each other.